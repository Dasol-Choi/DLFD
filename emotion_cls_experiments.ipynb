{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ed23c4-33f2-4705-82b2-4b3b186fa45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import gdown\n",
    "\n",
    "# file_id = '1wA0RNEJ8tLJ45DkMKFBj1yvKl9bb9zvX'\n",
    "# gdown.download(f'https://drive.google.com/uc?id={file_id}', 'RAF_DB.zip', quiet=False)\n",
    "# with zipfile.ZipFile('RAF_DB.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('RAF_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3348ff1d-b8b2-47e5-9bb6-ee3851c9291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import linear_model, model_selection\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from trainer import train, test\n",
    "from evaluation.mia import MIA\n",
    "from model import load_model\n",
    "from RAF_loader import get_data_loaders\n",
    "from utils import set_seed\n",
    "from evaluation.accuracy import calculate_accuracy, calculate_all_accuracies\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f335222-8cd3-491d-8c8c-2afe5de576ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 11044\n",
      "Train (Retain) dataset size: 7730\n",
      "Forget (Train) dataset size: 3314\n",
      "Validation dataset size: 1227\n",
      "Test dataset size: 3068\n",
      "Retain Test dataset size: 7730\n",
      "Forget Test dataset size: 3314\n",
      "=== Size of dataset ===\n",
      "Train dataset size: 11044\n",
      "Train (Retain) dataset size: 7730\n",
      "Forget (Train) dataset size: 3314\n",
      "Validation dataset size: 1227\n",
      "Test dataset size: 3068\n",
      "Retain Test dataset size: 7730\n",
      "Forget Test dataset size: 3314\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/dasol/Unlearning/RAF_DB/DATASET\"\n",
    "data_loaders = get_data_loaders(data_dir, batch_size=64)\n",
    "\n",
    "print(\"=== Size of dataset ===\")\n",
    "print(f\"Train dataset size: {len(data_loaders['train'].dataset)}\")\n",
    "print(f\"Train (Retain) dataset size: {len(data_loaders['retain_train'].dataset)}\")\n",
    "print(f\"Forget (Train) dataset size: {len(data_loaders['forget_train'].dataset)}\")\n",
    "print(f\"Validation dataset size: {len(data_loaders['val'].dataset)}\")\n",
    "print(f\"Test dataset size: {len(data_loaders['test'].dataset)}\")\n",
    "print(f\"Retain Test dataset size: {len(data_loaders['retain_test'].dataset)}\")\n",
    "print(f\"Forget Test dataset size: {len(data_loaders['forget_test'].dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518647a3-2ac9-4272-a4d6-75b3d4d2de14",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee342f8e-fc9b-4426-adca-b473817882e6",
   "metadata": {},
   "source": [
    "### Training Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b5e168-860c-4f20-95d9-7a9cad2e6dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(7, device, model_path=None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "history = []\n",
    "accuracy = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, data_loaders['train'], criterion, optimizer, scheduler, device, epoch)\n",
    "    val_loss, val_acc = test(model, data_loaders['val'], criterion, device)\n",
    "    history.append((train_loss, train_acc))\n",
    "    accuracy.append((val_loss, val_acc))\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        print(\"[Info] best val accuracy!\")\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        #torch.save(model.state_dict(), f'checkpoints/best_{epoch + 1}_emotion_resnet_original.pth')\n",
    "\n",
    "#torch.save(model.state_dict(), f'checkpoints/resnet/last_{num_epochs}_emotion_original.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab1fa54-ff0f-412d-84fb-8ac67ac53b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: {'Loss': 0.018342545638656223, 'Acc': 0.7503259452411994, 'Top-2 Acc': 0.8872229465449805}\n",
      "MIA: {'MIA Regression Accuracy': 0.6036706349206349, 'MIA CV Accuracy': 0.6741784037558685, 'Forgetting Score': 0.17417840375586846}\n",
      "Final score: 0.7009845688647313\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path =  'checkpoint/last_100_emotion_original.pth'\n",
    "model = load_model(7, device, model_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = calculate_accuracy(model, data_loaders['test'], criterion, device)\n",
    "mia = MIA(model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb5ec7-7db2-4627-bb3b-8924c7e9702f",
   "metadata": {},
   "source": [
    "### Training Retrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38095a53-54bf-43d6-a254-988d73efbd30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "retrained_model = load_model(7, device, model_path=None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(retrained_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=200)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "history = []\n",
    "accuracy = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(retrained_model, data_loaders['retain_train'], criterion, optimizer, scheduler, device, epoch)\n",
    "    val_loss, val_acc = test(retrained_model, data_loaders['val'], criterion, device)\n",
    "    history.append((train_loss, val_loss))\n",
    "    accuracy.append((train_acc, val_acc))\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        print(\"[Info] best val accuracy!\")\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        #torch.save(retrained_model.state_dict(), f'checkpoint/best_{epoch + 1}_emotion_retrained.pth')\n",
    "\n",
    "#torch.save(retrained_model.state_dict(), f'checkpoint/last_{num_epochs}_emotion_retrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dd89382-6bd9-442f-87be-d041e96a3038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: {'Loss': 0.017435058815382917, 'Acc': 0.719361147327249, 'Top-2 Acc': 0.8503911342894394}\n",
      "MIA: {'MIA Regression Accuracy': 0.6055839002267573, 'MIA CV Accuracy': 0.5195618153364631, 'Forgetting Score': 0.019561815336463062}\n",
      "Final score: 0.8401187583271614\n"
     ]
    }
   ],
   "source": [
    "model_path = f'checkpoint/last_100_emotion_retrained.pth'\n",
    "retrained_model = load_model(7, device, model_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = calculate_accuracy(retrained_model, data_loaders['test'], criterion, device)\n",
    "mia = MIA(retrained_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95179808-efd9-4860-b1cc-c5646eaaf498",
   "metadata": {},
   "source": [
    "## Unlearning Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18478577-102e-44f2-a2ea-dab58628da86",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bdea6b7-47c1-4960-a004-44d8f4afb291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1 - Training]\n",
      "[Batch: 80] running train loss: 0.0005701056293219153, running train accuracy: 0.991015613079071\n",
      "train loss: 0.0004860167033466743, accuracy: 0.9922380447387695\n",
      "elapsed time: 9.167838335037231\n",
      "[Test]\n",
      "[Batch: 1] running test loss: 0.02380838245153427, running test accuracy: 0.671875\n",
      "test loss: 0.02025782687934123, accuracy: 0.7416462898254395\n",
      "elapsed time: 1.211104393005371\n",
      "[Epoch: 2 - Training]\n",
      "[Batch: 80] running train loss: 0.0002695734336612077, running train accuracy: 0.9974609613418579\n",
      "train loss: 0.00025526780504395977, accuracy: 0.9975420236587524\n",
      "elapsed time: 9.124073505401611\n",
      "[Test]\n",
      "[Batch: 1] running test loss: 0.023482386022806168, running test accuracy: 0.671875\n",
      "test loss: 0.020534290418259576, accuracy: 0.7424612641334534\n",
      "elapsed time: 1.1604735851287842\n",
      "Test Acc: {'Loss': 0.01818828937819779, 'Acc': 0.7516297262059974, 'Top-2 Acc': 0.8895045632333768}\n",
      "MIA: {'MIA Regression Accuracy': 0.6203939909297053, 'MIA CV Accuracy': 0.6830985915492958, 'Forgetting Score': 0.18309859154929575}\n",
      "Final score: 0.692716271553703\n"
     ]
    }
   ],
   "source": [
    "model_path = 'checkpoint/last_100_emotion_original.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetune_model = load_model(7, device, model_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(finetune_model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "num_epochs=2\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(finetune_model, data_loaders['retain_train'], criterion, optimizer, scheduler, device, epoch)\n",
    "    val_loss, val_acc = test(finetune_model, data_loaders['val'], criterion, device)\n",
    "\n",
    "test_acc = calculate_accuracy(finetune_model, data_loaders['test'], criterion, device)\n",
    "mia = MIA(finetune_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb893df-e116-48b5-9c1b-51603b6ca295",
   "metadata": {},
   "source": [
    "### CF-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b458bfc-f31c-4bdc-baeb-0b4d6e90816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1 - Training]\n",
      "[Batch: 80] running train loss: 0.0005404932484907477, running train accuracy: 0.990234375\n",
      "train loss: 0.0005690344670765767, accuracy: 0.9900388121604919\n",
      "elapsed time: 7.364529848098755\n",
      "[Test]\n",
      "[Batch: 1] running test loss: 0.031077299267053604, running test accuracy: 0.625\n",
      "test loss: 0.018655992786653797, accuracy: 0.7513037919998169\n",
      "elapsed time: 2.8717427253723145\n",
      "Epoch [1/2] - Train Loss: 0.0006, Train Accuracy: 0.99%\n",
      "Epoch [1/2] - Test Loss: 0.0187, Test Accuracy: 0.75%\n",
      "[Epoch: 2 - Training]\n",
      "[Batch: 80] running train loss: 0.0005941680913110758, running train accuracy: 0.990039050579071\n",
      "train loss: 0.0005348306739604026, accuracy: 0.991073727607727\n",
      "elapsed time: 7.329920291900635\n",
      "[Test]\n",
      "[Batch: 1] running test loss: 0.0306535754352808, running test accuracy: 0.609375\n",
      "test loss: 0.018709906634924427, accuracy: 0.7513037919998169\n",
      "elapsed time: 2.8507800102233887\n",
      "Epoch [2/2] - Train Loss: 0.0005, Train Accuracy: 0.99%\n",
      "Epoch [2/2] - Test Loss: 0.0187, Test Accuracy: 0.75%\n",
      "Test Acc: {'Loss': 0.018709906634924427, 'Acc': 0.7513037809647979, 'Top-2 Acc': 0.8875488917861799}\n",
      "MIA: {'MIA Regression Accuracy': 0.6033163265306123, 'MIA CV Accuracy': 0.672926447574335, 'Forgetting Score': 0.17292644757433495}\n",
      "Final score: 0.7027254429080639\n"
     ]
    }
   ],
   "source": [
    "from unlearn.cfk import cfk_train\n",
    "\n",
    "model_path =  'checkpoint/last_100_emotion_original.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs=2\n",
    "cfk_model = load_model(7, device, model_path)\n",
    "cfk_model = cfk_train(cfk_model, data_loaders['retain_train'], data_loaders['test'], device, epochs)\n",
    "\n",
    "test_acc = calculate_accuracy(cfk_model, data_loaders['test'], criterion, device)\n",
    "mia = MIA(cfk_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3928160-e27d-4371-b4a6-ff48d8676552",
   "metadata": {},
   "source": [
    "### EU-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d62f38bb-e248-4d4d-902a-f81931da2a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlearning Epoch [1/2] - Loss: 0.0367, Accuracy: 98.95%\n",
      "Unlearning Epoch [2/2] - Loss: 0.0356, Accuracy: 98.95%\n",
      "Test Acc: {'Loss': 0.01820390381166685, 'Acc': 0.7522816166883963, 'Top-2 Acc': 0.8859191655801826}\n",
      "MIA: {'MIA Regression Accuracy': 0.6033163265306123, 'MIA CV Accuracy': 0.6740219092331767, 'Forgetting Score': 0.17402190923317673}\n",
      "Final score: 0.7021188991110214\n"
     ]
    }
   ],
   "source": [
    "from unlearn.euk import euk_unlearn\n",
    "\n",
    "epochs = 2\n",
    "model_path =  'checkpoint/last_100_emotion_original.pth'\n",
    "euk_model = load_model(7, device, model_path)\n",
    "euk_model = euk_unlearn(euk_model, data_loaders['retain_train'], device, epochs)\n",
    "\n",
    "test_acc = calculate_accuracy(euk_model, data_loaders['test'], criterion, device)\n",
    "mia = MIA(euk_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb1a68-b25f-43ea-acde-cdad908d0613",
   "metadata": {},
   "source": [
    "### NegGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7dd9fd9-5118-4c16-b609-6298de59f8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NegGrad Epoch [1/1] Loss: 0.059\n",
      "Test Acc: {'Loss': 0.019304277586314773, 'Acc': 0.7323989569752282, 'Top-2 Acc': 0.8715775749674055}\n",
      "MIA: {'MIA Regression Accuracy': 0.5943168934240363, 'MIA CV Accuracy': 0.6466353677621284, 'Forgetting Score': 0.14663536776212838}\n",
      "Final score: 0.7195641107254858\n"
     ]
    }
   ],
   "source": [
    "from unlearn.neg_grad import neggrad_unlearn\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.gpu = 0\n",
    "        self.lr = 0.001\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 5e-4\n",
    "        self.num_classes = 8\n",
    "        self.epochs = 1\n",
    "        self.device = torch.device(f\"cuda:{self.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args = Args()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_path =  'checkpoint/last_100_emotion_original.pth'\n",
    "neggrad_model = load_model(7, args.device, model_path)\n",
    "neggrad_unlearn(neggrad_model, data_loaders['forget_train'], criterion, args)\n",
    "\n",
    "test_acc = calculate_accuracy(neggrad_model, data_loaders['test'], criterion, args.device)\n",
    "mia = MIA(neggrad_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], args.device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f5822-47ca-4f6e-8176-1704148456fc",
   "metadata": {},
   "source": [
    "### UNSIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ad46dba-8bd1-47a6-8396-fa3bd0b26be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] - Noise Loss: 2921.9857\n",
      "Impair Step - Train loss 1: 2.3708, Train Acc: 34.76%\n",
      "Performance of Impaired Model on Forget Samples\n",
      "Test Acc: {'Loss': 0.025897050618814522, 'Acc': 0.386245110821382, 'Top-2 Acc': 0.6078878748370273}\n",
      "Impaired MIA : {'MIA Regression Accuracy': 0.5477607709750567, 'MIA CV Accuracy': 0.5195618153364631, 'Forgetting Score': 0.019561815336463062}\n",
      "Repair Step - Train loss 1: 1.3011, Train Acc: 51.89%\n",
      "Repair Step - Train loss 2: 2.3603, Train Acc: 115.01%\n",
      "Performance of Repaired Model on Forget Samples\n",
      "Test Acc: {'Loss': 0.016864737833338892, 'Acc': 0.6215775749674055, 'Top-2 Acc': 0.7920469361147328}\n",
      "Repaired MIA: {'MIA Regression Accuracy': 0.5479024943310657, 'MIA CV Accuracy': 0.5195618153364631, 'Forgetting Score': 0.019561815336463062}\n",
      "Final score: 0.7912269721472397\n"
     ]
    }
   ],
   "source": [
    "from unlearn.unsir import Noise, generate_error_maximizing_noise, impair_step, repair_step\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.1\n",
    "        self.num_classes = 8\n",
    "        self.device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.impair_epochs = 1\n",
    "        self.repair_epochs = 2\n",
    "        self.lr = 0.001\n",
    "\n",
    "args = Args()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_path = 'checkpoint/last_100_emotion_original.pth'\n",
    "unsir_model = load_model(7, args.device, model_path)\n",
    "\n",
    "noise = generate_error_maximizing_noise(unsir_model, data_loaders['forget_train'], args.device, args.impair_epochs)\n",
    "\n",
    "# Impair Step\n",
    "noisy_data = [(noise()[i % noise().size(0)], data_loaders['forget_train'].dataset[i][1]) for i in range(len(data_loaders['forget_train'].dataset))]\n",
    "noisy_loader = DataLoader(noisy_data, batch_size=128, shuffle=True)\n",
    "impair_step(unsir_model, noisy_loader, args.device, args.lr, args.impair_epochs)\n",
    "\n",
    "print(\"Performance of Impaired Model on Forget Samples\")\n",
    "test_acc = calculate_accuracy(unsir_model, data_loaders['test'], criterion, args.device)\n",
    "mia_impaired = MIA(unsir_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], args.device)\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Impaired MIA : {mia_impaired}')\n",
    "\n",
    "# Repair Step\n",
    "repair_step(unsir_model, data_loaders['retain_train'], args.device, args.lr, args.repair_epochs)\n",
    "\n",
    "print(\"Performance of Repaired Model on Forget Samples\")\n",
    "test_acc = calculate_accuracy(unsir_model, data_loaders['test'], criterion, args.device)\n",
    "mia_repaired = MIA(unsir_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], args.device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia_repaired[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'Repaired MIA: {mia_repaired}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83dd82a-6a09-4e17-972e-fd420a1661ff",
   "metadata": {},
   "source": [
    "### BadT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b26001ba-e2ed-4e8a-852f-45ccf179a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Bad Teacher Unlearning ...\n",
      "Epoch 1: train_acc: 53.60, train_loss: -55732.36\n",
      "Test Acc: {'Loss': 340.42474463040304, 'Acc': 0.5153194263363755, 'Top-2 Acc': 0.6711212516297262}\n",
      "MIA: {'MIA Regression Accuracy': 0.5477607709750567, 'MIA CV Accuracy': 0.5211267605633803, 'Forgetting Score': 0.021126760563380254}\n",
      "Final score: 0.7365329526048074\n"
     ]
    }
   ],
   "source": [
    "from unlearn.badT import badt\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.seed = 42\n",
    "        self.bt_optim = \"adam\"\n",
    "        self.bt_alpha = 1\n",
    "        self.bt_beta = 1\n",
    "        self.bt_kd_T = 4\n",
    "        self.bt_epochs = 1\n",
    "        self.bt_learning_rate = 0.001\n",
    "        self.bt_lr_decay_epochs = [10, 10, 10]\n",
    "        self.bt_lr_decay_rate = 0.1\n",
    "        self.bt_weight_decay = 5e-4\n",
    "        self.bt_momentum = 0.9\n",
    "\n",
    "args = Args()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")\n",
    "model_path = 'checkpoint/last_100_emotion_original.pth'\n",
    "gteacher = load_model(7, args.device, model_path)\n",
    "bteacher = load_model(7, args.device, model_path=None)\n",
    "student = load_model(7, args.device, model_path)\n",
    "\n",
    "badT_model = badt(\n",
    "    gteacher=gteacher,\n",
    "    bteacher=bteacher,\n",
    "    student=student,\n",
    "    retain_loader=data_loaders[\"retain_train\"],\n",
    "    forget_loader=data_loaders[\"forget_train\"],\n",
    "    valid_loader_full=data_loaders[\"test\"],\n",
    "    args=args\n",
    ")\n",
    "\n",
    "test_acc = calculate_accuracy(badT_model, data_loaders['test'], criterion, args.device)\n",
    "mia = MIA(badT_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], args.device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e3ac5-b419-4d6e-b887-6fd5a0de7b50",
   "metadata": {},
   "source": [
    "### SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f0114b1-184a-4528-90a8-8575ff5917da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n",
      "Epoch 1: maximize loss: -0.12, train_acc: 98.80, minimize loss: 0.11\n",
      "==> scrub unlearning ...\n",
      "Epoch 2: maximize loss: -0.22, train_acc: 99.02, minimize loss: 0.11\n",
      "Total time: 62.48 seconds\n",
      "Test Acc: {'Loss': 0.018813876191098266, 'Acc': 0.750651890482399, 'Top-2 Acc': 0.878748370273794}\n",
      "MIA: {'MIA Regression Accuracy': 0.6038832199546486, 'MIA CV Accuracy': 0.6375586854460094, 'Forgetting Score': 0.13755868544600935}\n",
      "Final score: 0.7377672597951901\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unlearn.scrub import scrub\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.seed = 42\n",
    "        self.optim = \"sgd\"  \n",
    "        self.sgda_epochs = 2\n",
    "        self.sgda_learning_rate = 0.0005\n",
    "        self.sgda_weight_decay = 5e-4\n",
    "        self.sgda_momentum = 0.9\n",
    "        self.msteps = 2\n",
    "        self.kd_T = 4  \n",
    "        self.lr_decay_epochs = [3, 5, 9]\n",
    "        self.lr_decay_rate = 0.1\n",
    "\n",
    "args = Args()\n",
    "model_path =  'checkpoint/last_100_emotion_original.pth'\n",
    "teacher_model = load_model(7, args.device, model_path)\n",
    "student_model = load_model(7, args.device, model_path)\n",
    "\n",
    "scrub_model = scrub(\n",
    "    teacher=teacher_model,\n",
    "    student=student_model,\n",
    "    args=args,\n",
    "    retain_loader_train=data_loaders[\"retain_train\"],\n",
    "    retain_loader_test=data_loaders[\"retain_test\"],\n",
    "    forget_loader_train=data_loaders[\"forget_train\"],\n",
    "    forget_loader_test=data_loaders[\"forget_test\"],\n",
    "    valid_loader_full=data_loaders[\"test\"] \n",
    ")\n",
    "\n",
    "test_acc = calculate_accuracy(scrub_model, data_loaders['test'], criterion, args.device)\n",
    "mia = MIA(scrub_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], args.device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA: {mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d52f4c-15ef-426f-960e-cbb84a6f8ff1",
   "metadata": {},
   "source": [
    "### DLFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "77c0db44-f924-4f9a-9107-d6e4ff9b4509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Forgetting Phase...\n",
      "Batch 1/121 - Test Acc: 0.7074, MIA Forgetting Score: 0.2826, Final Score: 0.5711\n",
      "Batch 2/121 - Test Acc: 0.6968, MIA Forgetting Score: 0.2690, Final Score: 0.5794\n",
      "Batch 3/121 - Test Acc: 0.6145, MIA Forgetting Score: 0.2475, Final Score: 0.5598\n",
      "Batch 4/121 - Test Acc: 0.5884, MIA Forgetting Score: 0.2398, Final Score: 0.5544\n",
      "Batch 5/121 - Test Acc: 0.5575, MIA Forgetting Score: 0.2396, Final Score: 0.5392\n",
      "Batch 6/121 - Test Acc: 0.5460, MIA Forgetting Score: 0.2332, Final Score: 0.5398\n",
      "Batch 7/121 - Test Acc: 0.6349, MIA Forgetting Score: 0.2382, Final Score: 0.5792\n",
      "Batch 8/121 - Test Acc: 0.5974, MIA Forgetting Score: 0.2367, Final Score: 0.5620\n",
      "Batch 9/121 - Test Acc: 0.5273, MIA Forgetting Score: 0.2336, Final Score: 0.5300\n",
      "Batch 10/121 - Test Acc: 0.5436, MIA Forgetting Score: 0.2341, Final Score: 0.5377\n",
      "Batch 11/121 - Test Acc: 0.5778, MIA Forgetting Score: 0.2297, Final Score: 0.5592\n",
      "New best model saved with Test Acc: 0.5778\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 12/121 - Test Acc: 0.5721, MIA Forgetting Score: 0.2297, Final Score: 0.5564\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 13/121 - Test Acc: 0.5852, MIA Forgetting Score: 0.2297, Final Score: 0.5629\n",
      "New best model saved with Test Acc: 0.5852\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 14/121 - Test Acc: 0.5966, MIA Forgetting Score: 0.2297, Final Score: 0.5686\n",
      "New best model saved with Test Acc: 0.5966\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 15/121 - Test Acc: 0.5705, MIA Forgetting Score: 0.2299, Final Score: 0.5554\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 16/121 - Test Acc: 0.5982, MIA Forgetting Score: 0.2301, Final Score: 0.5690\n",
      "Batch 17/121 - Test Acc: 0.5648, MIA Forgetting Score: 0.2297, Final Score: 0.5527\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 18/121 - Test Acc: 0.5664, MIA Forgetting Score: 0.2305, Final Score: 0.5527\n",
      "Batch 19/121 - Test Acc: 0.4523, MIA Forgetting Score: 0.2314, Final Score: 0.4947\n",
      "Batch 20/121 - Test Acc: 0.3757, MIA Forgetting Score: 0.2305, Final Score: 0.4573\n",
      "Batch 21/121 - Test Acc: 0.4026, MIA Forgetting Score: 0.2305, Final Score: 0.4708\n",
      "Batch 22/121 - Test Acc: 0.3757, MIA Forgetting Score: 0.2303, Final Score: 0.4575\n",
      "Batch 23/121 - Test Acc: 0.3284, MIA Forgetting Score: 0.2297, Final Score: 0.4346\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 24/121 - Test Acc: 0.6023, MIA Forgetting Score: 0.2297, Final Score: 0.5715\n",
      "New best model saved with Test Acc: 0.6023\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 25/121 - Test Acc: 0.5583, MIA Forgetting Score: 0.2297, Final Score: 0.5495\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 26/121 - Test Acc: 0.5550, MIA Forgetting Score: 0.2297, Final Score: 0.5478\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 27/121 - Test Acc: 0.5591, MIA Forgetting Score: 0.2299, Final Score: 0.5497\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 28/121 - Test Acc: 0.6153, MIA Forgetting Score: 0.2301, Final Score: 0.5776\n",
      "Batch 29/121 - Test Acc: 0.5534, MIA Forgetting Score: 0.2297, Final Score: 0.5470\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 30/121 - Test Acc: 0.5355, MIA Forgetting Score: 0.2297, Final Score: 0.5381\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 31/121 - Test Acc: 0.5591, MIA Forgetting Score: 0.2297, Final Score: 0.5499\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 32/121 - Test Acc: 0.5257, MIA Forgetting Score: 0.2301, Final Score: 0.5327\n",
      "Batch 33/121 - Test Acc: 0.4751, MIA Forgetting Score: 0.2301, Final Score: 0.5075\n",
      "Batch 34/121 - Test Acc: 0.4841, MIA Forgetting Score: 0.2301, Final Score: 0.5119\n",
      "Batch 35/121 - Test Acc: 0.5118, MIA Forgetting Score: 0.2305, Final Score: 0.5254\n",
      "Batch 36/121 - Test Acc: 0.5338, MIA Forgetting Score: 0.2308, Final Score: 0.5361\n",
      "Batch 37/121 - Test Acc: 0.5403, MIA Forgetting Score: 0.2303, Final Score: 0.5398\n",
      "Batch 38/121 - Test Acc: 0.5371, MIA Forgetting Score: 0.2303, Final Score: 0.5382\n",
      "Batch 39/121 - Test Acc: 0.5102, MIA Forgetting Score: 0.2297, Final Score: 0.5254\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 40/121 - Test Acc: 0.5738, MIA Forgetting Score: 0.2303, Final Score: 0.5565\n",
      "Batch 41/121 - Test Acc: 0.6007, MIA Forgetting Score: 0.2325, Final Score: 0.5678\n",
      "Batch 42/121 - Test Acc: 0.6104, MIA Forgetting Score: 0.2312, Final Score: 0.5740\n",
      "Batch 43/121 - Test Acc: 0.6186, MIA Forgetting Score: 0.2290, Final Score: 0.5803\n",
      "New best model saved with Test Acc: 0.6186\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 44/121 - Test Acc: 0.5990, MIA Forgetting Score: 0.2299, Final Score: 0.5696\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 45/121 - Test Acc: 0.5721, MIA Forgetting Score: 0.2321, Final Score: 0.5540\n",
      "Batch 46/121 - Test Acc: 0.5493, MIA Forgetting Score: 0.2316, Final Score: 0.5430\n",
      "Batch 47/121 - Test Acc: 0.5126, MIA Forgetting Score: 0.2319, Final Score: 0.5244\n",
      "Batch 48/121 - Test Acc: 0.4980, MIA Forgetting Score: 0.2319, Final Score: 0.5171\n",
      "Batch 49/121 - Test Acc: 0.5045, MIA Forgetting Score: 0.2319, Final Score: 0.5204\n",
      "Batch 50/121 - Test Acc: 0.5249, MIA Forgetting Score: 0.2314, Final Score: 0.5310\n",
      "Batch 51/121 - Test Acc: 0.5232, MIA Forgetting Score: 0.2319, Final Score: 0.5297\n",
      "Batch 53/121 - Test Acc: 0.5363, MIA Forgetting Score: 0.2319, Final Score: 0.5363\n",
      "Batch 54/121 - Test Acc: 0.5143, MIA Forgetting Score: 0.2321, Final Score: 0.5250\n",
      "Batch 55/121 - Test Acc: 0.5208, MIA Forgetting Score: 0.2321, Final Score: 0.5283\n",
      "Batch 56/121 - Test Acc: 0.5281, MIA Forgetting Score: 0.2319, Final Score: 0.5322\n",
      "Batch 57/121 - Test Acc: 0.5297, MIA Forgetting Score: 0.2314, Final Score: 0.5334\n",
      "Batch 58/121 - Test Acc: 0.5257, MIA Forgetting Score: 0.2319, Final Score: 0.5310\n",
      "Batch 59/121 - Test Acc: 0.5208, MIA Forgetting Score: 0.2319, Final Score: 0.5285\n",
      "Batch 60/121 - Test Acc: 0.5232, MIA Forgetting Score: 0.2323, Final Score: 0.5293\n",
      "Batch 61/121 - Test Acc: 0.5216, MIA Forgetting Score: 0.2319, Final Score: 0.5289\n",
      "Batch 62/121 - Test Acc: 0.5297, MIA Forgetting Score: 0.2319, Final Score: 0.5330\n",
      "Batch 63/121 - Test Acc: 0.5338, MIA Forgetting Score: 0.2319, Final Score: 0.5350\n",
      "Batch 64/121 - Test Acc: 0.5452, MIA Forgetting Score: 0.2321, Final Score: 0.5405\n",
      "Batch 65/121 - Test Acc: 0.5558, MIA Forgetting Score: 0.2323, Final Score: 0.5456\n",
      "Batch 66/121 - Test Acc: 0.5591, MIA Forgetting Score: 0.2325, Final Score: 0.5470\n",
      "Batch 67/121 - Test Acc: 0.5566, MIA Forgetting Score: 0.2325, Final Score: 0.5458\n",
      "Batch 68/121 - Test Acc: 0.5452, MIA Forgetting Score: 0.2321, Final Score: 0.5405\n",
      "Batch 69/121 - Test Acc: 0.5436, MIA Forgetting Score: 0.2321, Final Score: 0.5397\n",
      "Batch 70/121 - Test Acc: 0.5387, MIA Forgetting Score: 0.2321, Final Score: 0.5373\n",
      "Batch 71/121 - Test Acc: 0.5363, MIA Forgetting Score: 0.2323, Final Score: 0.5358\n",
      "Batch 72/121 - Test Acc: 0.5412, MIA Forgetting Score: 0.2319, Final Score: 0.5387\n",
      "Batch 73/121 - Test Acc: 0.5412, MIA Forgetting Score: 0.2316, Final Score: 0.5389\n",
      "Batch 74/121 - Test Acc: 0.5485, MIA Forgetting Score: 0.2303, Final Score: 0.5439\n",
      "Batch 75/121 - Test Acc: 0.5640, MIA Forgetting Score: 0.2299, Final Score: 0.5521\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 76/121 - Test Acc: 0.5949, MIA Forgetting Score: 0.2303, Final Score: 0.5671\n",
      "Batch 77/121 - Test Acc: 0.6007, MIA Forgetting Score: 0.2310, Final Score: 0.5693\n",
      "Batch 78/121 - Test Acc: 0.6055, MIA Forgetting Score: 0.2310, Final Score: 0.5718\n",
      "Batch 79/121 - Test Acc: 0.6349, MIA Forgetting Score: 0.2303, Final Score: 0.5871\n",
      "Batch 80/121 - Test Acc: 0.6292, MIA Forgetting Score: 0.2308, Final Score: 0.5838\n",
      "Batch 81/121 - Test Acc: 0.6121, MIA Forgetting Score: 0.2301, Final Score: 0.5759\n",
      "Batch 82/121 - Test Acc: 0.5998, MIA Forgetting Score: 0.2299, Final Score: 0.5700\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 83/121 - Test Acc: 0.6341, MIA Forgetting Score: 0.2325, Final Score: 0.5845\n",
      "Batch 84/121 - Test Acc: 0.6414, MIA Forgetting Score: 0.2332, Final Score: 0.5875\n",
      "Batch 85/121 - Test Acc: 0.6324, MIA Forgetting Score: 0.2319, Final Score: 0.5844\n",
      "Batch 86/121 - Test Acc: 0.5917, MIA Forgetting Score: 0.2316, Final Score: 0.5642\n",
      "Batch 87/121 - Test Acc: 0.5575, MIA Forgetting Score: 0.2308, Final Score: 0.5480\n",
      "Batch 88/121 - Test Acc: 0.5493, MIA Forgetting Score: 0.2305, Final Score: 0.5441\n",
      "Batch 89/121 - Test Acc: 0.5664, MIA Forgetting Score: 0.2305, Final Score: 0.5527\n",
      "Batch 90/121 - Test Acc: 0.5982, MIA Forgetting Score: 0.2321, Final Score: 0.5670\n",
      "Batch 91/121 - Test Acc: 0.6218, MIA Forgetting Score: 0.2325, Final Score: 0.5784\n",
      "Batch 92/121 - Test Acc: 0.6080, MIA Forgetting Score: 0.2325, Final Score: 0.5715\n",
      "Batch 93/121 - Test Acc: 0.6039, MIA Forgetting Score: 0.2327, Final Score: 0.5692\n",
      "Batch 94/121 - Test Acc: 0.6047, MIA Forgetting Score: 0.2330, Final Score: 0.5694\n",
      "Batch 95/121 - Test Acc: 0.6023, MIA Forgetting Score: 0.2327, Final Score: 0.5684\n",
      "Batch 96/121 - Test Acc: 0.6072, MIA Forgetting Score: 0.2327, Final Score: 0.5708\n",
      "Batch 97/121 - Test Acc: 0.6031, MIA Forgetting Score: 0.2323, Final Score: 0.5692\n",
      "Batch 98/121 - Test Acc: 0.6080, MIA Forgetting Score: 0.2319, Final Score: 0.5721\n",
      "Batch 99/121 - Test Acc: 0.6121, MIA Forgetting Score: 0.2316, Final Score: 0.5744\n",
      "Batch 100/121 - Test Acc: 0.6235, MIA Forgetting Score: 0.2314, Final Score: 0.5803\n",
      "Batch 101/121 - Test Acc: 0.6341, MIA Forgetting Score: 0.2303, Final Score: 0.5867\n",
      "Batch 102/121 - Test Acc: 0.6300, MIA Forgetting Score: 0.2299, Final Score: 0.5851\n",
      "New best model saved with Test Acc: 0.6300\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 103/121 - Test Acc: 0.6610, MIA Forgetting Score: 0.2299, Final Score: 0.6006\n",
      "New best model saved with Test Acc: 0.6610\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 105/121 - Test Acc: 0.6699, MIA Forgetting Score: 0.2314, Final Score: 0.6035\n",
      "Batch 106/121 - Test Acc: 0.6667, MIA Forgetting Score: 0.2305, Final Score: 0.6028\n",
      "Batch 107/121 - Test Acc: 0.6324, MIA Forgetting Score: 0.2303, Final Score: 0.5859\n",
      "Batch 108/121 - Test Acc: 0.6104, MIA Forgetting Score: 0.2297, Final Score: 0.5755\n",
      "Forgetting score below threshold. Fine-tuning with only classification loss.\n",
      "Batch 109/121 - Test Acc: 0.6365, MIA Forgetting Score: 0.2305, Final Score: 0.5877\n",
      "Batch 110/121 - Test Acc: 0.6626, MIA Forgetting Score: 0.2316, Final Score: 0.5996\n",
      "Batch 111/121 - Test Acc: 0.6707, MIA Forgetting Score: 0.2325, Final Score: 0.6028\n",
      "Batch 112/121 - Test Acc: 0.6748, MIA Forgetting Score: 0.2336, Final Score: 0.6038\n",
      "Batch 113/121 - Test Acc: 0.6699, MIA Forgetting Score: 0.2354, Final Score: 0.5996\n",
      "Batch 114/121 - Test Acc: 0.6642, MIA Forgetting Score: 0.2345, Final Score: 0.5976\n",
      "Batch 115/121 - Test Acc: 0.6650, MIA Forgetting Score: 0.2354, Final Score: 0.5971\n",
      "Batch 116/121 - Test Acc: 0.6610, MIA Forgetting Score: 0.2354, Final Score: 0.5951\n",
      "Batch 117/121 - Test Acc: 0.6487, MIA Forgetting Score: 0.2347, Final Score: 0.5896\n",
      "Batch 118/121 - Test Acc: 0.6430, MIA Forgetting Score: 0.2347, Final Score: 0.5868\n",
      "Batch 119/121 - Test Acc: 0.6463, MIA Forgetting Score: 0.2347, Final Score: 0.5884\n",
      "Batch 120/121 - Test Acc: 0.6528, MIA Forgetting Score: 0.2345, Final Score: 0.5919\n"
     ]
    }
   ],
   "source": [
    "from unlearn.dlfd import train_dlfd, set_seed, linear_weight_scheduler\n",
    "set_seed(42) \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = f'checkpoint/last_100_emotion_original.pth'\n",
    "dlfm_model = load_model(7, device, model_path)\n",
    "path = 'checkpoint/emotion_dlfd.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(dlfm_model.parameters(), lr=0.006, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "forget_weight = 7  # Weight for forgetting\n",
    "initial_retain_weight = 0.5  # Initial weight for retaining\n",
    "final_retain_weight = 1.0  # Final weight for retaining\n",
    "perturbation_steps = 20  # Number of steps for perturbation\n",
    "perturbation_strength = 2.0  # Strength of perturbation\n",
    "forget_threshold = 0.23  # Forgetting score threshold\n",
    "best_test_acc = 0  # Save the best test accuracy\n",
    "max_forgetting_score = 0.23  # Forgetting score saving threshold\n",
    "\n",
    "train_dlfd(\n",
    "        dlfm_model, data_loaders, criterion, optimizer, device,\n",
    "        perturbation_steps, perturbation_strength, forget_threshold, max_forgetting_score,\n",
    "        initial_retain_weight, final_retain_weight, forget_weight, path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e4bbb95f-6e1d-4d94-9466-de6f465e6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: {'Loss': 0.015832779987325414, 'Acc': 0.6606910039113429, 'Top-2 Acc': 0.809973924380704}\n",
      "MIA :{'MIA Regression Accuracy': 0.5508786848072562, 'MIA CV Accuracy': 0.5464788732394367, 'Forgetting Score': 0.04647887323943667}\n",
      "Final score: 0.7838666287162348\n"
     ]
    }
   ],
   "source": [
    "model_path = 'checkpoint/emotion_dlfd.pth'\n",
    "dlfm_model = load_model(7, device, model_path)\n",
    "\n",
    "test_acc = calculate_accuracy(dlfm_model, data_loaders['test'], criterion, device)\n",
    "mia = MIA(dlfm_model, data_loaders[\"retain_test\"], data_loaders['forget_test'], data_loaders['test'], device)\n",
    "final_score = (test_acc[\"Acc\"] + 1 - abs(mia[\"Forgetting Score\"] * 2)) / 2\n",
    "print(f'Test Acc: {test_acc}')\n",
    "print(f'MIA :{mia}')\n",
    "print(f'Final score: {final_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195753e7-72fc-428e-b2ce-d926ea0a9d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
